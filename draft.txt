
Model Explainability

Machine learning algorithms have enormous potential for space weather forecasting due to their fast deployment and ability to continuily improve
as new data becomes avalible via online/offline learning. One of the drawbacks of utilizing neural networks such as the one featured in this work
is the difficulty in determining the driving factors in the model's output. The so called black box or
opaque box problem presents a a practical issue for space weather forecasters who may need to know why a model outputs an elevated risk,
but more fundementally it does not allow us to determine whether these data-driven models align with our physical understanding of the underlying
processes. This could lead to models with misleading outputs and could inhibit our ability to make data driven discoveries. In response to the
increased use of machine learning in space weather there has been a movement towards using methods more attuned to explainability (such as one of
the many tree based models), or to utilize methods of interpreting model predictions (CITE IONG AND REDDY) such as SHapley
Additive exPlanation (SHAP) (cite Lundberg).

The SHAP method for determining feature importance is based on Shapley values (cite shapley) and uses cooperative game theory to determine the
marginal contribution of model input parameters. In theory it does this by using an additive feature method that retrains the model for all
input feature subsets. In assigning an importance value to each feature the method compares the retrained model of a subset including the feature,
and one trained with the same subset but the feature witheld. To account for the affect of the other features in teh set on withholding the feature
being examined it repeats this process for all possible subsets and features. The final determinination of the feature importance is determined by
takinga weighted average of all the individual importance values. In practice this is prohibitivly expensive as it requires retraining 2^N models
where N is the number of input features, which in our case would be either 1,024 for the solar wind model or 65,536 for the combined model.
Thanksfully a Shapley Sampling method can be used which applies sampling for the feature importance calculations and aproximates the effect
of withholding features by integrating over samples from the training data set. In doing this we can avoid having to retrain the models and
reduce the computational burden of perfroming 2^N replacements.

Feature importance can be either global or local. Global feature importance provides a general picture of how important a feature is for a model
over the entire training set, where a local feature importance determins the feature's contributions to a single prediction (CITE IONG). SHAP is
a model agnostic method that determins local feature importance using the game theory approach of Shapley values.
A note must be made here about how the feature independence assumption of the SHAP method could affect our feature importance results. If two
input features have a high correlation or dependency then the calculations of their marginal contributions to one another can be skewed. For
instance, if feature A and B are highly correlated, and feature A is being witheld from a subset containing feature B, the output of the model
may not change significantly due to the addition of feature A because the model already had the required information from feature B. In this case,
feature A may not recieve any importance from this marginal contribution. This can lead to feature A having an atrificially low or high global and
specific feature importance. Alternitivly the marginal contributions can be shared between the features through the permutations of the
different subsets, creating reletivly equal feature importances where one should be significantly higher. However, for this work we are primarily
concerned with the additional contributions of the ground magnetometer data when it is included as input with the solar wind data, and these two
data sets do not show significant correlations.